# @package _global_
# python src.train -m experiment=wang2022/equivariance_encouraged/rgroup_2022 --multirun
hydra:
  mode: "MULTIRUN"
  sweeper:
    params:
      model.net.alpha: 0.1,0.01,0.0001,0.000001
      data.equivariance_level: 0,5,9

defaults:
  - override /data: wang2022/equivariance_test
  - override /model: wang2022/rgroup_2022
  - override /callbacks: [early_stopping_rmse,model_checkpoint]
  - override /trainer: default


tags: ["wang2022", "alpha_experiment"]

seed: 0
trainer:
  max_epochs: 150
  accelerator: auto
  devices: auto
  min_epochs: 50

data:
  batch_size: 8

model:
  net:
    in_channels: 2
    out_channels: 2
    hidden_dim: 64
    alpha: 0
    vel_inp: False
  autoregressive_train: False
  # default optimizer params
  # default scheduler params

logger:
  wandb:
    tags: ${tags}
    group: "alpha_experiment"
    project: "wang2022"
    entity: "uva-dl2"
  aim:
    experiment: "wang2022/alpha_experiment"