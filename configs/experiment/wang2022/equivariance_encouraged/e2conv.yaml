# @package _global_
# To run this experiment, execute:
# python src.train -m experiment=wang2022/equivariance_encouraged/e2conv --multirun
hydra:
  mode: "MULTIRUN"
  sweeper:
    params:
      data.equivariance_level: 0,5,9

defaults:
  - override /data: wang2022/equivariance_test
  - override /model: wang2022/e2conv
  - override /callbacks: [early_stopping_rmse,model_checkpoint]
  - override /trainer: default


tags: ["wang2022", "alpha_experiment"]

seed: 0
trainer:
  max_epochs: 150
  accelerator: auto
  devices: auto
  min_epochs: 50

data:
  batch_size: 8

model:
  net:
    in_frames: 1
    out_frames: 1
    hidden_dim: 92 # ~600k params
  autoregressive_train: False
  # default optimizer params
  # default scheduler params

logger:
  wandb:
    tags: ${tags}
    group: "alpha_experiment"
    project: "wang2022"
    entity: "uva-dl2"
  aim:
    experiment: "wang2022/alpha_experiment"