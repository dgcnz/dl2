{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, input_length, mid, output_length, direc, task_list, sample_list, stack = False):\n",
    "        self.input_length = input_length\n",
    "        self.mid = mid\n",
    "        self.output_length = output_length\n",
    "        self.direc = direc\n",
    "        self.task_list = task_list\n",
    "        self.sample_list = sample_list\n",
    "        self.stack = stack\n",
    "        try:\n",
    "            self.data_lists = [torch.load(self.direc + \"/raw_data_\" + str(idx[0]) + \"_\" + str(idx[1]) + \".pt\") for idx in task_list]\n",
    "        except:\n",
    "            self.data_lists = [torch.load(self.direc + \"/raw_data_\" + str(idx) + \".pt\") for idx in task_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.task_list) * len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        task_idx = index // len(self.sample_list)\n",
    "        sample_idx = index % len(self.sample_list)        \n",
    "        y = self.data_lists[task_idx][(self.sample_list[sample_idx]+self.mid):(self.sample_list[sample_idx]+self.mid+self.output_length)] \n",
    "        if not self.stack:\n",
    "            x = self.data_lists[task_idx][(self.mid-self.input_length+self.sample_list[sample_idx]):(self.mid+self.sample_list[sample_idx])]\n",
    "        else:\n",
    "            x = self.data_lists[task_idx][(self.mid-self.input_length+self.sample_list[sample_idx]):(self.mid+self.sample_list[sample_idx])].reshape(-1, y.shape[-2], y.shape[-1])     \n",
    "        return x.float(), y.float()\n",
    "    \n",
    "def train_epoch(train_loader, model, optimizer, loss_function):\n",
    "    train_mse = []\n",
    "    for xx, yy in train_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        loss = 0\n",
    "        for y in yy.transpose(0,1):\n",
    "            im = model(xx)\n",
    "            xx = torch.cat([xx[:, im.shape[1]:], im], 1)\n",
    "            loss += loss_function(im, y)  \n",
    "        train_mse.append(loss.item()/yy.shape[1]) \n",
    "        try:\n",
    "            weight_constraint = loss_function(model.module.get_weight_constraint(), torch.tensor(0).float().cuda())\n",
    "            loss += weight_constraint\n",
    "        except:\n",
    "            pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_rmse = round(np.sqrt(np.mean(train_mse)),5)\n",
    "    return train_rmse\n",
    "\n",
    "def eval_epoch(valid_loader, model, loss_function):\n",
    "    valid_mse = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xx, yy in valid_loader:\n",
    "            xx = xx.to(device)\n",
    "            yy = yy.to(device)\n",
    "            loss = 0\n",
    "            ims = []\n",
    "            for y in yy.transpose(0,1):\n",
    "                im = model(xx)\n",
    "                xx = torch.cat([xx[:, im.shape[1]:], im], 1)\n",
    "                loss += loss_function(im, y)\n",
    "                ims.append(im.cpu().data.numpy())\n",
    "            ims = np.array(ims).transpose(1,0,2,3,4)\n",
    "            preds.append(ims)\n",
    "            trues.append(yy.cpu().data.numpy())\n",
    "            valid_mse.append(loss.item()/yy.shape[1])\n",
    "        preds = np.concatenate(preds, axis = 0)  \n",
    "        trues = np.concatenate(trues, axis = 0)  \n",
    "        valid_rmse = round(np.sqrt(np.mean(valid_mse)), 5)\n",
    "    return valid_rmse, preds, trues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import e2cnn\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from e2cnn.nn.modules.r2_conv.r2convolution import compute_basis_params\n",
    "from e2cnn.nn.modules.r2_conv.basisexpansion_singleblock import block_basisexpansion\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "######### Regular MLP ##########\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, h_size, w_size):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x.view(x.shape[0], -1))\n",
    "    \n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, h_size, w_size, hidden_dim, num_layers):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.layers = [MLPBlock(in_channels*h_size*w_size, hidden_dim, h_size, w_size)]\n",
    "        self.layers += [MLPBlock(hidden_dim, hidden_dim, h_size, w_size) for i in range(num_layers-2)]\n",
    "        self.layers += [nn.Linear(hidden_dim, out_channels*h_size*w_size)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        self.w_size = w_size\n",
    "        self.h_size = h_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x.view(x.shape[0], -1)).reshape(x.shape[0], -1, self.h_size, self.w_size)\n",
    "         \n",
    "\n",
    "########### Regular CNN ########\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv =nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size-1)//2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layers = [ConvBlock(in_channels, hidden_dim, kernel_size)]\n",
    "        self.layers += [ConvBlock(hidden_dim, hidden_dim, kernel_size) for i in range(num_layers-2)]\n",
    "        self.layers += [nn.Conv2d(hidden_dim, out_channels, kernel_size, padding=(kernel_size-1)//2)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "\n",
    "######################################################  \n",
    "############ Rotationally Equivariant CNN ############\n",
    "######################################################\n",
    "\n",
    "\n",
    "class E2Conv(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, N):\n",
    "        super(E2Conv, self).__init__()\n",
    "        \n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.regular_repr])\n",
    "        feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.regular_repr])\n",
    "        \n",
    "        self.layer = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(feat_type_in, feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2),\n",
    "            e2cnn.nn.InnerBatchNorm(feat_type_hid),\n",
    "            e2cnn.nn.ReLU(feat_type_hid)\n",
    "        ) \n",
    "        \n",
    "    def forward(self, xx):\n",
    "        return self.layer(xx)\n",
    "\n",
    "class E2CNN(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, hidden_dim, kernel_size, num_layers, N):\n",
    "        super(E2CNN, self).__init__()\n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        \n",
    "        self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.irrep(1)])\n",
    "        self.feat_type_hid = e2cnn.nn.FieldType(r2_act, hidden_dim*[r2_act.regular_repr])\n",
    "        self.feat_type_out = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.irrep(1)])\n",
    "        \n",
    "        input_layer = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(self.feat_type_in, self.feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2),\n",
    "            e2cnn.nn.InnerBatchNorm(self.feat_type_hid),\n",
    "            e2cnn.nn.ReLU(self.feat_type_hid)\n",
    "        ) \n",
    "        \n",
    "        layers = [input_layer]\n",
    "        layers += [E2Conv(hidden_dim, hidden_dim, kernel_size, N) for i in range(num_layers-2)]\n",
    "        layers += [e2cnn.nn.R2Conv(self.feat_type_hid, self.feat_type_out, kernel_size = kernel_size, padding = (kernel_size - 1)//2)]\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "        \n",
    "    def forward(self, xx):\n",
    "        xx = e2cnn.nn.GeometricTensor(xx, self.feat_type_in)\n",
    "        out = self.model(xx)\n",
    "        return out.tensor\n",
    "    \n",
    "    \n",
    "########################################  \n",
    "############ Lift_Expansion ############\n",
    "########################################\n",
    "\n",
    "class Lift_Rot_Expansion(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, encoder_hidden_dim, backbone_hidden_dim, N):\n",
    "        super(Lift_Rot_Expansion, self).__init__() \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_frames*2*62*23, encoder_hidden_dim),\n",
    "            nn.BatchNorm1d(encoder_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoder_hidden_dim, encoder_hidden_dim),\n",
    "            nn.BatchNorm1d(encoder_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoder_hidden_dim, backbone_hidden_dim*N//4)\n",
    "        )\n",
    "\n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.irrep(1)])\n",
    "        self.feat_type_hid = e2cnn.nn.FieldType(r2_act, backbone_hidden_dim*[r2_act.regular_repr]) \n",
    "        self.feat_type_hid_2 = e2cnn.nn.FieldType(r2_act, (backbone_hidden_dim + backbone_hidden_dim//4)*[r2_act.regular_repr]) \n",
    "        self.feat_type_out = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.irrep(1)])\n",
    "        \n",
    "        self.e2conv_1 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(self.feat_type_in, self.feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2),\n",
    "            e2cnn.nn.InnerBatchNorm(self.feat_type_hid),\n",
    "            e2cnn.nn.ReLU(self.feat_type_hid),\n",
    "            e2cnn.nn.R2Conv(self.feat_type_hid, self.feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2),\n",
    "            e2cnn.nn.InnerBatchNorm(self.feat_type_hid),\n",
    "            e2cnn.nn.ReLU(self.feat_type_hid)\n",
    "        ) \n",
    "        \n",
    "        self.e2conv_2 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(self.feat_type_hid_2, self.feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2),\n",
    "            e2cnn.nn.InnerBatchNorm(self.feat_type_hid),\n",
    "            e2cnn.nn.ReLU(self.feat_type_hid),\n",
    "            e2cnn.nn.R2Conv(self.feat_type_hid, self.feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2),\n",
    "            e2cnn.nn.InnerBatchNorm(self.feat_type_hid),\n",
    "            e2cnn.nn.ReLU(self.feat_type_hid),\n",
    "            e2cnn.nn.R2Conv(self.feat_type_hid, self.feat_type_out, kernel_size = kernel_size, padding = (kernel_size - 1)//2)\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_out = self.encoder(x.reshape(x.shape[0], -1))\n",
    "        encoder_out = encoder_out.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "\n",
    "        out = e2cnn.nn.GeometricTensor(x, self.feat_type_in)\n",
    "        out = self.e2conv_1(out)\n",
    "        out = torch.cat([out.tensor, encoder_out], dim = 1)\n",
    "        out = e2cnn.nn.GeometricTensor(out, self.feat_type_hid_2)\n",
    "        out = self.e2conv_2(out)\n",
    "        return out.tensor\n",
    "    \n",
    "\n",
    "############ ConvNet + E2CNN ############\n",
    "\n",
    "class ConvE2CNN(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, conv_hidden_dim, e2cnn_hidden_dim, conv_num_layers, e2cnn_num_layers,  N):\n",
    "        super(ConvE2CNN, self).__init__()\n",
    "        \n",
    "        self.convnet = [ConvBlock(in_frames*2, conv_hidden_dim, kernel_size)]\n",
    "        self.convnet += [ConvBlock(conv_hidden_dim, conv_hidden_dim, kernel_size) for i in range(conv_num_layers-2)]\n",
    "        self.convnet += [ConvBlock(conv_hidden_dim, e2cnn_hidden_dim*N, kernel_size)]\n",
    "        self.convnet = nn.Sequential(*self.convnet)\n",
    "        \n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        self.feat_type_hid = e2cnn.nn.FieldType(r2_act, e2cnn_hidden_dim*[r2_act.regular_repr])\n",
    "        self.feat_type_out = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.irrep(1)])\n",
    "        \n",
    "        self.e2cnn = [E2Conv(e2cnn_hidden_dim, e2cnn_hidden_dim, kernel_size, N) for i in range(e2cnn_num_layers-2)]\n",
    "        self.e2cnn += [e2cnn.nn.R2Conv(self.feat_type_hid, self.feat_type_out, kernel_size = kernel_size, padding = (kernel_size - 1)//2)]\n",
    "        self.e2cnn = torch.nn.Sequential(*self.e2cnn)\n",
    "    \n",
    "        \n",
    "    def forward(self, xx):\n",
    "        out = self.convnet(xx)\n",
    "        out = e2cnn.nn.GeometricTensor(out, self.feat_type_hid)\n",
    "        out = self.e2cnn(out)\n",
    "        return out.tensor\n",
    "    \n",
    "\n",
    "    \n",
    "############ Rotational Residual Pathway ############\n",
    "\n",
    "class RPPBlock(nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, N, first_layer = False, final_layer = False):\n",
    "        super(RPPBlock, self).__init__()\n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        self.first_layer = first_layer\n",
    "        self.final_layer = final_layer\n",
    "        \n",
    "        # E2 Equivariant Layer\n",
    "        if self.first_layer:\n",
    "            self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.irrep(1)])\n",
    "        else:\n",
    "            self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.regular_repr])\n",
    "            \n",
    "        if self.final_layer:\n",
    "            self.feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.irrep(1)])\n",
    "        else:\n",
    "            self.feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.regular_repr])\n",
    "        self.e2cnn = e2cnn.nn.R2Conv(self.feat_type_in, self.feat_type_hid, kernel_size = kernel_size, padding = (kernel_size - 1)//2)\n",
    "        \n",
    "        # Regular Convolution Layer\n",
    "        if self.first_layer:\n",
    "            self.conv = nn.Conv2d(in_frames*2, out_frames*N, kernel_size, padding=(kernel_size-1)//2)\n",
    "        elif self.final_layer:\n",
    "            self.conv = nn.Conv2d(in_frames*N, out_frames*2, kernel_size, padding=(kernel_size-1)//2)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_frames*N, out_frames*N, kernel_size, padding=(kernel_size-1)//2)\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(out_frames*N)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        convout = self.conv(x)\n",
    "        e2cnnout = self.e2cnn(e2cnn.nn.GeometricTensor(x, self.feat_type_in)).tensor\n",
    "        if self.final_layer:\n",
    "            return convout + e2cnnout\n",
    "        else:\n",
    "            out = convout + e2cnnout\n",
    "            return self.activation(self.norm(out))\n",
    "    \n",
    "class Rot_RPPNet(nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, N, hidden_dim, num_layers):\n",
    "        super(Rot_RPPNet, self).__init__()\n",
    "        self.layers = [RPPBlock(in_frames = in_frames, out_frames = hidden_dim, kernel_size = kernel_size, \n",
    "                                N = N, first_layer = True, final_layer = False)]\n",
    "        self.layers += [RPPBlock(in_frames = hidden_dim, out_frames = hidden_dim, kernel_size = kernel_size, \n",
    "                                 N = N, first_layer = False, final_layer = False) for i in range(num_layers-2)]\n",
    "        self.layers += [RPPBlock(in_frames = hidden_dim, out_frames = out_frames, kernel_size = kernel_size, \n",
    "                                 N = N, first_layer = False, final_layer = True)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "        \n",
    "    def get_weight_constraint(self, conv_wd = 1e-6):\n",
    "        conv_l2 = 0.\n",
    "        basic_l2 = 0.\n",
    "        for block in self.model:\n",
    "            if hasattr(block, 'conv'):\n",
    "                conv_l2 += sum([p.pow(2).sum() for p in block.conv.parameters()])\n",
    "        return conv_wd*conv_l2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "  \n",
    "############ Constrained Locally Connected NN ############\n",
    "\n",
    "class Constrained_LCBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, h_size, w_size, final_layer = False):\n",
    "        super(Constrained_LCBlock, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.weights = nn.Parameter(torch.randn(h_size, w_size, out_channels, in_channels, kernel_size, kernel_size).float().to(device)/in_channels)\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels).float().to(device)/in_channels)\n",
    "        #self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_size = (kernel_size-1)//2\n",
    "        self.h_size = h_size\n",
    "        self.w_size = w_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.final_layer = final_layer\n",
    "        \n",
    "    def rot_vector(self, inp, theta):\n",
    "        #inp shape: c x 2 x 64 x 64\n",
    "        theta = torch.tensor(theta).float().to(device)\n",
    "        rot_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]).float().to(device)\n",
    "        out = torch.einsum(\"ab, bc... -> ac...\",(rot_matrix, inp.transpose(0,1))).transpose(0,1)\n",
    "        return out\n",
    "    \n",
    "    def get_rot_mat(self, theta):\n",
    "        theta = torch.tensor(theta).float().to(device)\n",
    "        return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "                             [torch.sin(theta), torch.cos(theta), 0]]).float().to(device)\n",
    "\n",
    "    def rot_img(self, x, theta):\n",
    "        rot_mat = self.get_rot_mat(theta)[None, ...].float().repeat(x.shape[0],1,1)\n",
    "        grid = F.affine_grid(rot_mat, x.size()).float()\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x.float()\n",
    "    \n",
    "    def get_rotated_kernels(self, theta):\n",
    "        temp_w = self.weights.reshape(self.h_size, self.w_size, self.out_channels, self.in_channels//2, 2, self.kernel_size, self.kernel_size)#.clone()\n",
    "        temp_w = temp_w.reshape(-1, 2, self.kernel_size, self.kernel_size)\n",
    "        temp_w = self.rot_vector(temp_w, theta)\n",
    "        temp_w = temp_w.reshape(-1, 2, self.kernel_size, self.kernel_size)\n",
    "        temp_w = torch.cat([self.rot_img(temp_w[:,:1], theta), self.rot_img(temp_w[:,1:2], theta)], dim = 1)\n",
    "        temp_w = temp_w.reshape(self.h_size, self.w_size, self.out_channels//2, 2, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        temp_w = temp_w.reshape(-1, 2, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        temp_w = self.rot_vector(temp_w, theta)\n",
    "        temp_w = temp_w.reshape(self.h_size, self.w_size, self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        return temp_w\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.unfold(x, kernel_size = self.kernel_size, padding = (self.kernel_size-1)//2)\n",
    "        x = x.reshape(x.shape[0], self.in_channels, self.kernel_size, self.kernel_size, -1)\n",
    "        x = x.reshape(x.shape[0], self.in_channels, self.kernel_size, self.kernel_size, self.h_size, self.w_size)\n",
    "        \n",
    "        # Compute Convolution: h x w x o x c x k x k and bz x c x k x k x h x w\n",
    "        out = torch.einsum(\"ijabcd, rbcdij -> raij\", self.weights, x)\n",
    "\n",
    "        \n",
    "        if self.final_layer:\n",
    "            return out\n",
    "        else:\n",
    "            return self.activation(out)\n",
    "        \n",
    "class Constrained_Rot_LCNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_dim, kernel_size, h_size, w_size, num_layers, alpha = 1, N = 4):\n",
    "        super(Constrained_Rot_LCNet, self).__init__()\n",
    "        layers = [Constrained_LCBlock(in_channels, hidden_dim, kernel_size, h_size, w_size)]\n",
    "        layers += [Constrained_LCBlock(hidden_dim, hidden_dim, kernel_size, h_size, w_size) for i in range(num_layers-2)]\n",
    "        layers += [Constrained_LCBlock(hidden_dim, out_channels, kernel_size, h_size, w_size, final_layer = True)]\n",
    "        self.rconv = nn.Sequential(*layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.alpha = alpha\n",
    "        self.N = N\n",
    "        \n",
    "    def get_weight_constraint(self): \n",
    "        constraint = 0\n",
    "        theta_step = np.pi*2/self.N\n",
    "        for layer in self.rconv:\n",
    "            for j in range(1, self.N):\n",
    "                constraint += F.mse_loss(layer.weights, layer.get_rotated_kernels(theta_step*j))\n",
    "                #print(F.mse_loss(layer.weights, layer.get_rotated_kernels(theta_step*j)).item())\n",
    "        return self.alpha*constraint\n",
    "          \n",
    "    def forward(self, x):\n",
    "        return self.rconv(x)\n",
    "    \n",
    "    \n",
    " \n",
    "############ Relaxed Group Convolution ############\n",
    "class Relaxed_LiftingConvolution(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 group_order,\n",
    "                 num_filter_banks,\n",
    "                 activation = True\n",
    "                 ):\n",
    "        super(Relaxed_LiftingConvolution, self).__init__()\n",
    "\n",
    "        self.num_filter_banks = num_filter_banks\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.group_order = group_order\n",
    "        self.activation = activation\n",
    "\n",
    "        self.combination_weights = torch.nn.Parameter(torch.ones(num_filter_banks, group_order).float()/num_filter_banks)\n",
    "\n",
    "        # Initialize an unconstrained kernel.\n",
    "        self.weight = torch.nn.Parameter(torch.zeros(self.num_filter_banks, # Additional dimension\n",
    "                                                     self.out_channels,\n",
    "                                                     self.in_channels,\n",
    "                                                     self.kernel_size,\n",
    "                                                     self.kernel_size))\n",
    "        stdv = np.sqrt(1/(self.in_channels*self.kernel_size*self.kernel_size))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        # If combination_weights are equal values, then the model is still equivariant\n",
    "        # self.combination_weights.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def generate_filter_bank(self):\n",
    "        \"\"\" Obtain a stack of rotated filters\"\"\"\n",
    "        weights = self.weight.reshape(self.num_filter_banks*self.out_channels,\n",
    "                                      self.in_channels,\n",
    "                                      self.kernel_size,\n",
    "                                      self.kernel_size)\n",
    "        filter_bank = torch.stack([rot_img(weights, -np.pi*2/self.group_order*i)\n",
    "                                   for i in range(self.group_order)])\n",
    "        filter_bank = filter_bank.transpose(0,1).reshape(self.num_filter_banks, # Additional dimension\n",
    "                                                         self.out_channels,\n",
    "                                                         self.group_order,\n",
    "                                                         self.in_channels,\n",
    "                                                         self.kernel_size,\n",
    "                                                         self.kernel_size)\n",
    "        return filter_bank\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: [bz, #in, h, w]\n",
    "        # output shape: [bz, #out, group order, h, w]\n",
    "\n",
    "        # generate filter bank given input group order\n",
    "        filter_bank = self.generate_filter_bank()\n",
    "\n",
    "        # for each rotation, we have a linear combination of multiple filters with different coefficients.\n",
    "        relaxed_conv_weights = torch.einsum(\"na, noa... -> oa...\", self.combination_weights, filter_bank)\n",
    "\n",
    "        # concatenate the first two dims before convolution.\n",
    "        # ==============================\n",
    "        x = F.conv2d(\n",
    "            input=x,\n",
    "            weight=relaxed_conv_weights.reshape(\n",
    "                self.out_channels * self.group_order,\n",
    "                self.in_channels,\n",
    "                self.kernel_size,\n",
    "                self.kernel_size\n",
    "            ),\n",
    "            padding = (self.kernel_size-1)//2\n",
    "        )\n",
    "        # ==============================\n",
    "\n",
    "        # reshape output signal to shape [bz, #out, group order, h, w].\n",
    "        # ==============================\n",
    "        x = x.view(\n",
    "            x.shape[0],\n",
    "            self.out_channels,\n",
    "            self.group_order,\n",
    "            x.shape[-1],\n",
    "            x.shape[-2]\n",
    "        )\n",
    "        # ==============================\n",
    "\n",
    "        if self.activation:\n",
    "            return F.relu(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "class Relaxed_GroupConv(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 group_order,\n",
    "                 num_filter_banks,\n",
    "                 activation = True\n",
    "                ):\n",
    "\n",
    "        super(Relaxed_GroupConv, self).__init__()\n",
    "\n",
    "        self.num_filter_banks = num_filter_banks\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.group_order = group_order\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "        ## Initialize weights\n",
    "        self.combination_weights = torch.nn.Parameter(torch.ones(group_order, num_filter_banks).float()/num_filter_banks/group_order)\n",
    "        self.weight = torch.nn.Parameter(torch.randn(self.num_filter_banks, ##additional dimension\n",
    "                                                       self.out_channels,\n",
    "                                                       self.in_channels,\n",
    "                                                       self.group_order,\n",
    "                                                       self.kernel_size,\n",
    "                                                       self.kernel_size))\n",
    "\n",
    "        stdv = np.sqrt(1/(self.in_channels*self.kernel_size*self.kernel_size))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        # If combination_weights are equal values, then the model is still equivariant\n",
    "        # self.combination_weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def generate_filter_bank(self):\n",
    "        \"\"\" Obtain a stack of rotated and cyclic shifted filters\"\"\"\n",
    "        filter_bank = []\n",
    "        weights = self.weight.reshape(self.num_filter_banks*self.out_channels*self.in_channels,\n",
    "                                      self.group_order,\n",
    "                                      self.kernel_size,\n",
    "                                      self.kernel_size)\n",
    "\n",
    "        for i in range(self.group_order):\n",
    "            # planar rotation\n",
    "            rotated_filter = rot_img(weights, -np.pi*2/self.group_order*i)\n",
    "\n",
    "            # cyclic shift\n",
    "            shifted_indices = torch.roll(torch.arange(0, self.group_order, 1), shifts = i)\n",
    "            shifted_rotated_filter = rotated_filter[:,shifted_indices]\n",
    "            \n",
    "            \n",
    "            filter_bank.append(shifted_rotated_filter.reshape(self.num_filter_banks,\n",
    "                                                    self.out_channels,\n",
    "                                                    self.in_channels,\n",
    "                                                    self.group_order,\n",
    "                                                    self.kernel_size,\n",
    "                                                    self.kernel_size))\n",
    "        # stack\n",
    "        filter_bank = torch.stack(filter_bank).permute(1,2,0,3,4,5,6)\n",
    "        return filter_bank\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        filter_bank = self.generate_filter_bank()\n",
    "\n",
    "        relaxed_conv_weights = torch.einsum(\"na, aon... -> on...\", self.combination_weights, filter_bank)\n",
    "\n",
    "        x = torch.nn.functional.conv2d(\n",
    "            input=x.reshape(\n",
    "                x.shape[0],\n",
    "                x.shape[1] * x.shape[2],\n",
    "                x.shape[3],\n",
    "                x.shape[4]\n",
    "                ),\n",
    "            weight=relaxed_conv_weights.reshape(\n",
    "                self.out_channels * self.group_order,\n",
    "                self.in_channels * self.group_order,\n",
    "                self.kernel_size,\n",
    "                self.kernel_size\n",
    "            ),\n",
    "            padding = (self.kernel_size-1)//2\n",
    "        )\n",
    "\n",
    "                # Reshape signal back [bz, #out * g_order, h, w] -> [bz, out, g_order, h, w]\n",
    "        x = x.view(x.shape[0], self.out_channels, self.group_order, x.shape[-2], x.shape[-1])\n",
    "        # ========================\n",
    "\n",
    "        return x\n",
    "\n",
    "class Relaxed_LiftingConvolution(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 group_order,\n",
    "                 num_filter_banks,\n",
    "                 activation = True\n",
    "                 ):\n",
    "        super(Relaxed_LiftingConvolution, self).__init__()\n",
    "\n",
    "        self.num_filter_banks = num_filter_banks\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.group_order = group_order\n",
    "        self.activation = activation\n",
    "\n",
    "        self.combination_weights = torch.nn.Parameter(torch.ones(num_filter_banks, group_order).float()/num_filter_banks)\n",
    "\n",
    "        # Initialize an unconstrained kernel.\n",
    "        self.weight = torch.nn.Parameter(torch.zeros(self.num_filter_banks, # Additional dimension\n",
    "                                                     self.out_channels,\n",
    "                                                     self.in_channels,\n",
    "                                                     self.kernel_size,\n",
    "                                                     self.kernel_size))\n",
    "        stdv = np.sqrt(1/(self.in_channels*self.kernel_size*self.kernel_size))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        # If combination_weights are equal values, then the model is still equivariant\n",
    "        # self.combination_weights.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def generate_filter_bank(self):\n",
    "        \"\"\" Obtain a stack of rotated filters\"\"\"\n",
    "        weights = self.weight.reshape(self.num_filter_banks*self.out_channels,\n",
    "                                      self.in_channels,\n",
    "                                      self.kernel_size,\n",
    "                                      self.kernel_size)\n",
    "        filter_bank = torch.stack([rot_img(weights, -np.pi*2/self.group_order*i)\n",
    "                                   for i in range(self.group_order)])\n",
    "        filter_bank = filter_bank.transpose(0,1).reshape(self.num_filter_banks, # Additional dimension\n",
    "                                                         self.out_channels,\n",
    "                                                         self.group_order,\n",
    "                                                         self.in_channels,\n",
    "                                                         self.kernel_size,\n",
    "                                                         self.kernel_size)\n",
    "        return filter_bank\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: [bz, #in, h, w]\n",
    "        # output shape: [bz, #out, group order, h, w]\n",
    "\n",
    "        # generate filter bank given input group order\n",
    "        filter_bank = self.generate_filter_bank()\n",
    "\n",
    "        # for each rotation, we have a linear combination of multiple filters with different coefficients.\n",
    "        relaxed_conv_weights = torch.einsum(\"na, noa... -> oa...\", self.combination_weights, filter_bank)\n",
    "\n",
    "        # concatenate the first two dims before convolution.\n",
    "        # ==============================\n",
    "        x = F.conv2d(\n",
    "            input=x,\n",
    "            weight=relaxed_conv_weights.reshape(\n",
    "                self.out_channels * self.group_order,\n",
    "                self.in_channels,\n",
    "                self.kernel_size,\n",
    "                self.kernel_size\n",
    "            ),\n",
    "            padding = (self.kernel_size-1)//2\n",
    "        )\n",
    "        # ==============================\n",
    "\n",
    "        # reshape output signal to shape [bz, #out, group order, h, w].\n",
    "        # ==============================\n",
    "        x = x.view(\n",
    "            x.shape[0],\n",
    "            self.out_channels,\n",
    "            self.group_order,\n",
    "            x.shape[-1],\n",
    "            x.shape[-2]\n",
    "        )\n",
    "        # ==============================\n",
    "\n",
    "        if self.activation:\n",
    "            return F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class RelaxedGroupEquivariantCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, hidden_dim, group_order, num_gconvs, num_filter_banks, vel_inp = True):\n",
    "        super().__init__()\n",
    "\n",
    "        # First transform \\rho_1 to regular representations. \n",
    "        theta = torch.tensor(2*np.pi/group_order).float()\n",
    "        self.lift_coefs = torch.tensor([[torch.cos(theta*i), torch.sin(theta*i)] for i in range(group_order)]).float().to(device)\n",
    "        \n",
    "        if vel_inp:\n",
    "            self.gconvs = [Relaxed_GroupConv(in_channels = in_channels,\n",
    "                                            out_channels = hidden_dim,\n",
    "                                            kernel_size = kernel_size,\n",
    "                                            group_order = group_order,\n",
    "                                            num_filter_banks = num_filter_banks,\n",
    "                                            activation = True)]\n",
    "        else:\n",
    "            self.gconvs = [Relaxed_LiftingConvolution(in_channels = in_channels,\n",
    "                                                      out_channels = hidden_dim,\n",
    "                                                      kernel_size = kernel_size,\n",
    "                                                      group_order = group_order,\n",
    "                                                      num_filter_banks = num_filter_banks,\n",
    "                                                      activation = True)]\n",
    "\n",
    "        for i in range(num_gconvs-2):\n",
    "            self.gconvs.append(Relaxed_GroupConv(in_channels = hidden_dim,\n",
    "                                                out_channels = hidden_dim,\n",
    "                                                kernel_size = kernel_size,\n",
    "                                                group_order = group_order,\n",
    "                                                num_filter_banks = num_filter_banks,\n",
    "                                                activation = True))\n",
    "            \n",
    "        self.gconvs.append(Relaxed_GroupConv(in_channels = hidden_dim,\n",
    "                                            out_channels = out_channels,\n",
    "                                            kernel_size = kernel_size,\n",
    "                                            group_order = group_order,\n",
    "                                            num_filter_banks = num_filter_banks,\n",
    "                                            activation = False))\n",
    "\n",
    "        self.gconvs = torch.nn.Sequential(*self.gconvs)\n",
    "\n",
    "\n",
    "        self.vel_inp = vel_inp\n",
    "        self.group_order = group_order\n",
    "\n",
    "    def forward(self, x, target_length = 1):\n",
    "        if self.vel_inp and len(x.shape) == 4:\n",
    "            x = x.reshape(x.shape[0], x.shape[1]//2, 2, x.shape[2], x.shape[3])\n",
    "        preds = []\n",
    "        for i in range(target_length):\n",
    "            if self.vel_inp:\n",
    "                x = torch.einsum(\"bivhw, nv->binhw\", x, self.lift_coefs)\n",
    "            out = self.gconvs(x)\n",
    "            if self.vel_inp:\n",
    "                out = torch.einsum(\"binhw, nv->bivhw\", out, self.lift_coefs)\n",
    "            else:\n",
    "                out = out.mean(2)\n",
    "            x = torch.cat([x[:, 1:], out], 1)\n",
    "            preds.append(out)\n",
    "            \n",
    "        outs = torch.cat(preds, dim=1)\n",
    "        outs = outs.reshape(outs.shape[0], -1, outs.shape[-2], outs.shape[-1])\n",
    "        return outs\n",
    "\n",
    "    \n",
    "def rot_img(x, theta):\n",
    "    \"\"\" Rotate 2D images\n",
    "    Args:\n",
    "        x : input images with shape [N, C, H, W]\n",
    "        theta: angle\n",
    "    Returns:\n",
    "        rotated images\n",
    "    \"\"\"\n",
    "    # Rotation Matrix (2 x 3)\n",
    "    rot_mat = torch.FloatTensor([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                 [np.sin(theta), np.cos(theta), 0]]).to(x.device)\n",
    "\n",
    "    # The affine transformation matrices should have the shape of N x 2 x 3\n",
    "    rot_mat = rot_mat.repeat(x.shape[0],1,1)\n",
    "\n",
    "    # Obtain transformed grid\n",
    "    # grid is the coordinates of pixels for rotated image\n",
    "    # F.affine_grid assumes the origin is in the middle\n",
    "    # and it rotates the positions of the coordinates\n",
    "    # r(f(x)) = f(r^-1 x)\n",
    "    grid = F.affine_grid(rot_mat, x.size(), align_corners=False).float().to(x.device)\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x\n",
    "\n",
    "def rot_vector(x, theta):\n",
    "    #x has the shape [c x 2 x h x w]\n",
    "    rho = torch.FloatTensor([[np.cos(theta), -np.sin(theta)],\n",
    "                             [np.sin(theta), np.cos(theta)]])\n",
    "    out = torch.einsum(\"ab, bc... -> ac...\",(rho, x.transpose(0,1))).transpose(0,1)\n",
    "    return out\n",
    "    \n",
    "# class Relaxed_Reg_GroupConv(nn.Module):\n",
    "#     def __init__(self, in_reps, out_reps, kernel_size, N, num_filter_banks, first_layer = False, final_layer = False):\n",
    "#         super(Relaxed_Reg_GroupConv, self).__init__()\n",
    "#         self.num_filter_banks = num_filter_banks\n",
    "#         self.N = N\n",
    "        \n",
    "#         self.first_layer = first_layer\n",
    "#         self.final_layer = final_layer\n",
    "\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.out_reps = out_reps\n",
    "#         self.in_reps = in_reps\n",
    "        \n",
    "#         stdv = np.sqrt(1/(self.in_reps))\n",
    "        \n",
    "#         ## If this is first layer, rho_1 -> rho_reg\n",
    "#         if self.first_layer:\n",
    "#             self.transform_weights = nn.Parameter(torch.ones(in_reps, 1).float().to(device)/2)\n",
    "#             #self.transform_weights.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "#         if self.final_layer == True:\n",
    "#             self.final_w = nn.Parameter(torch.randn(out_reps).to(device))\n",
    "#             self.final_w.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#         ## Initialize weights\n",
    "#         self.combination_weights = nn.Parameter(torch.ones(self.N, self.num_filter_banks).float().to(device)/self.num_filter_banks)\n",
    "#         self.combination_weights.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "#         self.weights = nn.Parameter(torch.randn(self.num_filter_banks, out_reps, self.N, in_reps, self.N, kernel_size, kernel_size).to(device))\n",
    "#         self.weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#         self.bias = nn.Parameter(torch.zeros(out_reps, self.N).to(device))\n",
    "#         self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "#         self.batch_norm = nn.BatchNorm2d(out_reps*self.N)\n",
    "        \n",
    "#     def permute(self, weights, bias):\n",
    "#         augmented_weights = []\n",
    "#         augmented_bias = []\n",
    "        \n",
    "#         for i in range(self.N):\n",
    "#             permuted_indices = list(np.roll(np.arange(0, self.N, 1), shift = i))\n",
    "\n",
    "#             temp_w = weights[i, :, :, :, permuted_indices,...][:, permuted_indices]\n",
    "#             temp_w = temp_w.reshape(self.out_reps*self.N, self.in_reps, self.N, self.kernel_size, self.kernel_size) \n",
    "#             temp_w = temp_w.reshape(self.out_reps*self.N, self.in_reps*self.N, self.kernel_size, self.kernel_size) \n",
    "#             temp_b = bias[:, permuted_indices]\n",
    "            \n",
    "#             augmented_weights.append(temp_w)\n",
    "#             augmented_bias.append(temp_b.reshape(-1))\n",
    "            \n",
    "#         return torch.cat(augmented_weights, dim = 0), torch.cat(augmented_bias, dim = 0)\n",
    "    \n",
    "#     def rot_vector(self, inp, theta):\n",
    "#         #inp shape: c x 2 x 64 x 64\n",
    "#         theta = torch.tensor(theta).float().to(device)\n",
    "#         rot_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]).float().to(device)\n",
    "#         out = torch.einsum(\"ab, bc... -> ac...\",(rot_matrix, inp.transpose(0,1))).transpose(0,1)\n",
    "#         return out\n",
    "\n",
    "#     def rot_img(self, x, theta):\n",
    "#         theta = torch.tensor(theta).float().to(device)\n",
    "#         get_rot_mat = torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "#                              [torch.sin(theta), torch.cos(theta), 0]]).float().to(device)\n",
    "#         rot_mat = get_rot_mat[None, ...].float().repeat(x.shape[0],1,1)\n",
    "#         grid = F.affine_grid(rot_mat, x.size()).float()\n",
    "#         x = F.grid_sample(x, grid)\n",
    "#         return x.float()\n",
    " \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         if self.first_layer:\n",
    "#             xs = []\n",
    "#             x = x.reshape(x.shape[0], x.shape[1]//2, 2, x.shape[-2], x.shape[-1])\n",
    "#             theta = torch.tensor(2*np.pi/self.N).float()\n",
    "#             for i in range(self.N):\n",
    "#                 lift = torch.tensor([torch.cos(theta*i), torch.sin(theta*i)]).float().to(device)\n",
    "#                 lift_weights = torch.einsum(\"ab, b -> ab\", self.transform_weights.repeat(1,2), lift)\n",
    "#                 xs.append(torch.einsum(\"abcde, bc -> abde\", x, lift_weights).unsqueeze(2))\n",
    "#             xs = torch.cat(xs, dim = 2)\n",
    "#             xs = xs.reshape(xs.shape[0], xs.shape[1]*xs.shape[2], xs.shape[3], xs.shape[4])\n",
    "#         else:\n",
    "#             xs = x\n",
    "        \n",
    "#         conv_weights = torch.einsum(\"na, ab... -> nb...\", self.combination_weights.to(self.weights.device), self.weights)\n",
    "#         augmented_weights, augmented_biases = self.permute(conv_weights, self.bias) \n",
    "        \n",
    "#         out = F.conv2d(xs, augmented_weights, augmented_biases, padding = (self.kernel_size - 1)//2)\n",
    "#         out = out.reshape(out.shape[0], self.N, self.out_reps*self.N, out.shape[-2], out.shape[-1]).mean(1)\n",
    "#         if self.final_layer == True:\n",
    "#             theta = torch.tensor(2*np.pi/self.N).float()\n",
    "#             out = out.reshape(out.shape[0], self.out_reps, self.N, out.shape[-2], out.shape[-1])\n",
    "#             out_u = torch.sum(torch.stack([out[:,:,i:i+1]*torch.cos(theta*i) for i in range(self.N)]), dim = 0)\n",
    "#             out_v = torch.sum(torch.stack([out[:,:,i:i+1]*torch.sin(theta*i) for i in range(self.N)]), dim = 0)\n",
    "#             out = torch.cat([out_u, out_v], dim  = 2)\n",
    "#             out = torch.einsum(\"abcde, b -> abcde\", out, self.final_w)\n",
    "#             out = out.reshape(out.shape[0], self.out_reps*2, out.shape[-2], out.shape[-1])\n",
    "#             return out\n",
    "#         else:\n",
    "#             return F.relu(out)#)self.batch_norm(\n",
    "\n",
    "# class Relaxed_Reg_GroupConvNet(nn.Module):\n",
    "#     def __init__(self, in_reps, out_reps, hidden_dim, kernel_size, num_layers, num_filter_banks, N):\n",
    "#         super(Relaxed_Reg_GroupConvNet, self).__init__()     \n",
    "        \n",
    "#         layers = [Relaxed_Reg_GroupConv(in_reps = in_reps, out_reps = hidden_dim, kernel_size = kernel_size, \n",
    "#                                     N = N, num_filter_banks = num_filter_banks, first_layer = True, final_layer = False)]\n",
    "        \n",
    "#         layers += [Relaxed_Reg_GroupConv(in_reps = hidden_dim, out_reps = hidden_dim, kernel_size = kernel_size, \n",
    "#                                      N = N, num_filter_banks = num_filter_banks, first_layer = False, final_layer = False) for i in range(num_layers-2)]\n",
    "        \n",
    "#         layers += [Relaxed_Reg_GroupConv(in_reps = hidden_dim, out_reps = out_reps, kernel_size = kernel_size, \n",
    "#                                      N = N, num_filter_banks = num_filter_banks, first_layer = False, final_layer = True)]\n",
    "        \n",
    "#         self.rconv = nn.Sequential(*layers)\n",
    "#     def rot_vector(self, inp, theta):\n",
    "#         #inp shape: c x 2 x 64 x 64\n",
    "#         theta = torch.tensor(theta).float().to(device)\n",
    "#         rot_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]).float().to(device)\n",
    "#         out = torch.einsum(\"ab, bc... -> ac...\",(rot_matrix, inp.transpose(0,1))).transpose(0,1)\n",
    "#         return out\n",
    "\n",
    "#     def rot_img(self, x, theta):\n",
    "#         theta = torch.tensor(theta).float().to(device)\n",
    "#         get_rot_mat = torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "#                              [torch.sin(theta), torch.cos(theta), 0]]).float().to(device)\n",
    "#         rot_mat = get_rot_mat[None, ...].float().repeat(x.shape[0],1,1)\n",
    "#         grid = F.affine_grid(rot_mat, x.size()).float()\n",
    "#         x = F.grid_sample(x, grid)\n",
    "#         return x.float()\n",
    "                  \n",
    "#     def forward(self, x):\n",
    "#         return self.rconv(x)\n",
    "    \n",
    "\n",
    " \n",
    "############ Relaxed Steerable Convolution ############\n",
    "\n",
    "class Relaxed_Rot_SteerConv(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, N, first_layer = False, last_layer = False):\n",
    "        super(Relaxed_Rot_SteerConv, self).__init__()\n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        self.last_layer = last_layer\n",
    "        self.first_layer = first_layer\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if self.first_layer:\n",
    "            self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.irrep(1)])\n",
    "        else:\n",
    "            self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.regular_repr])\n",
    "            \n",
    "        if self.last_layer:\n",
    "            self.feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.irrep(1)])\n",
    "        else:\n",
    "            self.feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.regular_repr])\n",
    "            \n",
    "        if not last_layer:\n",
    "            self.norm = e2cnn.nn.InnerBatchNorm(self.feat_type_hid)\n",
    "            self.relu = e2cnn.nn.ReLU(self.feat_type_hid)\n",
    "        \n",
    "        \n",
    "        grid, basis_filter, rings, sigma, maximum_frequency = compute_basis_params(kernel_size = kernel_size)\n",
    "        i_repr = self.feat_type_in._unique_representations.pop()\n",
    "        o_repr = self.feat_type_hid._unique_representations.pop()\n",
    "        basis = self.feat_type_in.gspace.build_kernel_basis(i_repr, o_repr, sigma, rings, maximum_frequency = 5)\n",
    "        block_expansion = block_basisexpansion(basis, grid, basis_filter, recompute=False)\n",
    "\n",
    "        \n",
    "        self.basis_kernels = block_expansion.sampled_basis.to(device)\n",
    "        \n",
    "        \n",
    "        stdv = np.sqrt(1/(in_frames*kernel_size*kernel_size))\n",
    "        self.relaxed_weights = nn.Parameter(torch.ones(out_frames, self.basis_kernels.shape[0], in_frames, kernel_size**2).float().to(device))\n",
    "        self.relaxed_weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(out_frames*self.basis_kernels.shape[1]).to(device))\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        # self.relaxed_weights = nn.Parameter(torch.ones(out_frames, self.basis_kernels.shape[0], in_frames, kernel_size**2).float().to(device)/self.basis_kernels.shape[0]/(kernel_size**2))\n",
    "        # self.bias = nn.Parameter(torch.zeros(out_frames*self.basis_kernels.shape[1]).to(device))\n",
    "\n",
    "        \n",
    "    def get_weight_constraint(self):\n",
    "        return torch.mean(torch.abs(self.relaxed_weights[...,:-1] - self.relaxed_weights[...,1:])) #torch.roll()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_filters = torch.einsum('bpqk,obik->opiqk', self.basis_kernels.to(self.relaxed_weights.device), self.relaxed_weights) \n",
    "        conv_filters = conv_filters.reshape(conv_filters.shape[0]*conv_filters.shape[1],\n",
    "                                            conv_filters.shape[2]*conv_filters.shape[3], \n",
    "                                            self.kernel_size, self.kernel_size)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            out = F.conv2d(x, conv_filters, self.bias, padding = 1)\n",
    "            return self.relu(e2cnn.nn.GeometricTensor(out, self.feat_type_hid)).tensor#self.norm(\n",
    "        else:\n",
    "            return F.conv2d(x, conv_filters, self.bias, padding = 1)\n",
    "        \n",
    "class Relaxed_Rot_SteerConvNet(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, hidden_dim, kernel_size, num_layers, N, alpha = 1):\n",
    "        super(Relaxed_Rot_SteerConvNet, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "        layers = [Relaxed_Rot_SteerConv(in_frames = in_frames, out_frames = hidden_dim, \n",
    "                                 kernel_size = kernel_size, N = N, \n",
    "                                 first_layer = True, last_layer = False)]\n",
    "        \n",
    "        layers += [Relaxed_Rot_SteerConv(in_frames = hidden_dim, out_frames = hidden_dim, \n",
    "                                  kernel_size = kernel_size, N = N, \n",
    "                                  first_layer = False, last_layer = False) \n",
    "                   for i in range(num_layers-2)]\n",
    "        \n",
    "        layers += [Relaxed_Rot_SteerConv(in_frames = hidden_dim, out_frames = out_frames, \n",
    "                                  kernel_size = kernel_size, N = N, \n",
    "                                  first_layer = False, last_layer = True) ]\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def rot_vector(self, inp, theta):\n",
    "        #inp shape: c x 2 x 64 x 64\n",
    "        theta = torch.tensor(theta).float().to(device)\n",
    "        rot_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]).float().to(device)\n",
    "        out = torch.einsum(\"ab, bc... -> ac...\",(rot_matrix, inp.transpose(0,1))).transpose(0,1)\n",
    "        return out\n",
    "    \n",
    "    def get_rot_mat(self, theta):\n",
    "        theta = torch.tensor(theta).float().to(device)\n",
    "        return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "                             [torch.sin(theta), torch.cos(theta), 0]]).float().to(device)\n",
    "\n",
    "    def rot_img(self, x, theta):\n",
    "        rot_mat = self.get_rot_mat(theta)[None, ...].float().repeat(x.shape[0],1,1)\n",
    "        grid = F.affine_grid(rot_mat, x.size()).float()\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x.float()\n",
    "   \n",
    "    def get_weight_constraint(self):\n",
    "        return self.alpha * sum([layer.get_weight_constraint() for layer in self.model])\n",
    "    \n",
    "        \n",
    "    def forward(self, xx):\n",
    "        return self.model(xx)\n",
    "\n",
    "    \n",
    "    \n",
    "############ Relaxed Rotation and Translation Steerable Convolution ############\n",
    "\n",
    "class Relaxed_R_SteerConv(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, N, first_layer = False, last_layer = False):\n",
    "        super(Relaxed_R_SteerConv, self).__init__()\n",
    "        r2_act = e2cnn.gspaces.Rot2dOnR2(N = N)\n",
    "        self.last_layer = last_layer\n",
    "        self.first_layer = first_layer\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if self.first_layer:\n",
    "            self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.irrep(1)]) \n",
    "        else:\n",
    "            self.feat_type_in = e2cnn.nn.FieldType(r2_act, in_frames*[r2_act.regular_repr])\n",
    "            \n",
    "        if self.last_layer:\n",
    "            self.feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.irrep(1)])\n",
    "        else:\n",
    "            self.feat_type_hid = e2cnn.nn.FieldType(r2_act, out_frames*[r2_act.regular_repr]) \n",
    "            \n",
    "        if not last_layer:\n",
    "            self.norm = e2cnn.nn.InnerBatchNorm(self.feat_type_hid)\n",
    "            self.relu = e2cnn.nn.ReLU(self.feat_type_hid)\n",
    "        \n",
    "        \n",
    "        grid, basis_filter, rings, sigma, maximum_frequency = compute_basis_params(kernel_size = kernel_size)\n",
    "        i_repr = self.feat_type_in._unique_representations.pop()\n",
    "        o_repr = self.feat_type_hid._unique_representations.pop()\n",
    "        basis = self.feat_type_in.gspace.build_kernel_basis(i_repr, o_repr, sigma, rings, maximum_frequency = 5)\n",
    "        block_expansion = block_basisexpansion(basis, grid, basis_filter, recompute=False)\n",
    "\n",
    "        \n",
    "        self.basis_kernels = block_expansion.sampled_basis.to(device)  \n",
    "        \n",
    "        \n",
    "        stdv = np.sqrt(1/(in_frames*kernel_size*kernel_size))\n",
    "        self.relaxed_weights = nn.Parameter(torch.ones(out_frames, self.basis_kernels.shape[0], in_frames, kernel_size**2).float().to(device))\n",
    "        self.relaxed_weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(out_frames*self.basis_kernels.shape[1]).to(device))\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def get_weight_constraint(self):\n",
    "        return torch.mean(torch.abs(self.relaxed_weights[...,:-1] - self.relaxed_weights[...,1:])) #torch.roll()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_filters = torch.einsum('bpqk,obik->opiqk', self.basis_kernels.to(self.relaxed_weights.device), self.relaxed_weights) \n",
    "        conv_filters = conv_filters.reshape(conv_filters.shape[0]*conv_filters.shape[1],\n",
    "                                            conv_filters.shape[2]*conv_filters.shape[3], \n",
    "                                            self.kernel_size, self.kernel_size)\n",
    "        \n",
    "        return F.conv2d(x, conv_filters, self.bias, padding = 1)\n",
    "        \n",
    "class Relaxed_TR_SteerConv(nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, kernel_size, N, num_banks, h_size, w_size, first_layer = False, last_layer = False):\n",
    "        super(Relaxed_TR_SteerConv, self).__init__()\n",
    "        self.convs = nn.Sequential(*[Relaxed_R_SteerConv(in_frames = in_frames, out_frames = out_frames, \n",
    "                                                       kernel_size = kernel_size, N = N, first_layer = first_layer, \n",
    "                                                       last_layer = last_layer).to(device) for i in range(num_banks)])\n",
    "        \n",
    "        self.combination_weights = nn.Parameter(torch.ones(h_size, w_size, num_banks).float().to(device)/num_banks)\n",
    "        \n",
    "        #self.activation = nn.ReLU()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_size = (kernel_size-1)//2\n",
    "        self.h_size = h_size\n",
    "        self.w_size = w_size\n",
    "        self.last_layer = last_layer\n",
    "        self.num_banks = num_banks\n",
    "            \n",
    "\n",
    "    def get_weight_constraint(self):\n",
    "        return sum([layer.get_weight_constraint() for layer in self.convs])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = torch.stack([self.convs[i](x) for i in range(self.num_banks)], dim  = 0)\n",
    "        \n",
    "        # Compute Convolution\n",
    "        out = torch.einsum(\"ijr, rboij -> boij\", self.combination_weights, outs)\n",
    "        \n",
    "        \n",
    "        if self.last_layer:\n",
    "            return out\n",
    "        else:\n",
    "            return self.convs[0].relu(e2cnn.nn.GeometricTensor(out, self.convs[0].feat_type_hid)).tensor\n",
    "        \n",
    "        \n",
    "class Relaxed_TR_SteerConvNet(torch.nn.Module):\n",
    "    def __init__(self, in_frames, out_frames, hidden_dim, kernel_size, num_layers, N, num_banks, h_size, w_size, alpha = 1):\n",
    "        super(Relaxed_TR_SteerConvNet, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "        layers = [Relaxed_TR_SteerConv(in_frames = in_frames, out_frames = hidden_dim, \n",
    "                                       kernel_size = kernel_size, N = N, num_banks = num_banks,\n",
    "                                       h_size = h_size, w_size = w_size, first_layer = True, last_layer = False)]\n",
    "        \n",
    "        layers += [Relaxed_TR_SteerConv(in_frames = hidden_dim, out_frames = hidden_dim, \n",
    "                                        kernel_size = kernel_size, N = N, num_banks = num_banks,\n",
    "                                        h_size = h_size, w_size = w_size, first_layer = False, last_layer = False) \n",
    "                   for i in range(num_layers-2)]\n",
    "        \n",
    "        layers += [Relaxed_TR_SteerConv(in_frames = hidden_dim, out_frames = out_frames, \n",
    "                                        kernel_size = kernel_size, N = N, num_banks = num_banks,\n",
    "                                        h_size = h_size, w_size = w_size, first_layer = False, last_layer = True) ]\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def rot_vector(self, inp, theta):\n",
    "        #inp shape: c x 2 x 64 x 64\n",
    "        theta = torch.tensor(theta).float().to(device)\n",
    "        rot_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]).float().to(device)\n",
    "        out = torch.einsum(\"ab, bc... -> ac...\",(rot_matrix, inp.transpose(0,1))).transpose(0,1)\n",
    "        return out\n",
    "    \n",
    "    def get_rot_mat(self, theta):\n",
    "        theta = torch.tensor(theta).float().to(device)\n",
    "        return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "                             [torch.sin(theta), torch.cos(theta), 0]]).float().to(device)\n",
    "\n",
    "    def rot_img(self, x, theta):\n",
    "        rot_mat = self.get_rot_mat(theta)[None, ...].float().repeat(x.shape[0],1,1)\n",
    "        grid = F.affine_grid(rot_mat, x.size()).float()\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x.float()\n",
    "   \n",
    "    def get_weight_constraint(self):\n",
    "        return self.alpha * sum([layer.get_weight_constraint() for layer in self.model])\n",
    "    \n",
    "        \n",
    "    def forward(self, xx):\n",
    "        return self.model(xx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4 test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Equiv Errors: | [0.0, 0.17, 0.313, 0.435, 0.541, 0.63, 0.709, 0.78, 0.84, 0.896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "ERROR:tornado.general:SEND Error: Host unreachable\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/dgcnz/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/dgcnz/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                          ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^  File \"/Users/dgcnz/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "^\n",
      "  File \"/Users/dgcnz/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^AttributeError^: ^Can't get attribute 'Dataset' on <module '__main__' (built-in)>^^\n",
      "^^\n",
      "AttributeError: Can't get attribute 'Dataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 53975) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/development/uva/dl2/dl2-project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/connection.py:947\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 947\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/development/uva/dl2/dl2-project/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 53975) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    148\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 149\u001b[0m train_rmse\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    151\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    152\u001b[0m mse, preds, trues \u001b[38;5;241m=\u001b[39m eval_epoch(valid_loader, model, loss_fun)\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_epoch\u001b[39m(train_loader, model, optimizer, loss_function):\n\u001b[1;32m     42\u001b[0m     train_mse \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43myy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/uva/dl2/dl2-project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/development/uva/dl2/dl2-project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/development/uva/dl2/dl2-project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/development/uva/dl2/dl2-project/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 53975) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from torch.utils import data\n",
    "#from models import E2CNN, Relaxed_Rot_SteerConvNet, ConvNet\n",
    "#from utils import Dataset, train_epoch, eval_epoch, get_lr\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"Data Equiv Errors: | {}\".format([0.0, 0.17, 0.313, 0.435, 0.541, 0.63, 0.709, 0.78, 0.84, 0.896]))\n",
    "\n",
    "hidden_dim = 64\n",
    "num_layers = 5\n",
    "out_length = 6\n",
    "alpha = 0 \n",
    "input_length = 1\n",
    "batch_size = 32\n",
    "num_epoch = 1000\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.9\n",
    "mid = input_length + 2\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Split time ranges\n",
    "train_time = list(range(0, 30))\n",
    "valid_time = list(range(30, 40))\n",
    "\n",
    "def rot_vector(inp, theta):\n",
    "    #inp shape: c x 2 x 64 x 64\n",
    "    theta = torch.tensor(theta).float().to(device)\n",
    "    rot_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]).float().to(device)\n",
    "    out = torch.einsum(\"ab, bc... -> ac...\",(rot_matrix, inp.transpose(0,1))).transpose(0,1)\n",
    "    return out\n",
    "\n",
    "def get_rot_mat(theta):\n",
    "    theta = torch.tensor(theta).float().to(device)\n",
    "    return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "                         [torch.sin(theta), torch.cos(theta), 0]]).float().to(device)\n",
    "\n",
    "def rot_img(x, theta):\n",
    "    rot_mat = get_rot_mat(theta)[None, ...].float().repeat(x.shape[0],1,1)\n",
    "    grid = F.affine_grid(rot_mat, x.size()).float()\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x.float()\n",
    "\n",
    "def rot_field(x, theta):\n",
    "    x_rot = torch.cat([rot_img(rot_vector(x, theta)[:,:1],  theta), \n",
    "                       rot_img(rot_vector(x, theta)[:,-1:], theta)], dim = 1)\n",
    "    return x_rot\n",
    "\n",
    "\n",
    "for model_name in [\"E2CNN\", \"ConvNet\", \"RSteer\"]:#\n",
    "    equiv_error_lst = []\n",
    "    for level in range(1):\n",
    "        min_rmse = 1e8\n",
    "        data_direc = \"../data/smokeplume/equivariance_test/E_\" + str(level)\n",
    "        from pathlib import Path\n",
    "        \n",
    "        print(Path(data_direc).exists())\n",
    "        train_task = [0, 1, 2, 3]\n",
    "\n",
    "        train_set = Dataset(input_length = input_length, \n",
    "                            mid = mid, \n",
    "                            output_length = out_length,\n",
    "                            direc = data_direc, \n",
    "                            task_list = train_task, \n",
    "                            sample_list = train_time, \n",
    "                            stack = True)\n",
    "\n",
    "        valid_set = Dataset(input_length = input_length, \n",
    "                            mid = mid, \n",
    "                            output_length = out_length, \n",
    "                            direc = data_direc, \n",
    "                            task_list = train_task,\n",
    "                            sample_list = valid_time, \n",
    "                            stack = True)\n",
    "\n",
    "        train_loader = data.DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "        valid_loader = data.DataLoader(valid_set, batch_size = batch_size, shuffle = False, num_workers = 2) \n",
    "        test_loader = data.DataLoader(valid_set, batch_size = 1, shuffle = False, num_workers = 2)    \n",
    "\n",
    "\n",
    "        if model_name == \"E2CNN\":\n",
    "            model = nn.DataParallel(E2CNN(in_frames= input_length, \n",
    "                                          out_frames = 1, \n",
    "                                          hidden_dim = hidden_dim,\n",
    "                                          kernel_size = 3, \n",
    "                                          num_layers = num_layers,\n",
    "                                          N = 4).to(device))\n",
    "\n",
    "        elif model_name == \"RSteer\":\n",
    "            model = nn.DataParallel(Relaxed_Rot_SteerConvNet(in_frames = input_length, \n",
    "                                                             out_frames = 1, \n",
    "                                                             hidden_dim = hidden_dim//2, \n",
    "                                                             kernel_size = 3, \n",
    "                                                             num_layers = num_layers, \n",
    "                                                             N = 4, \n",
    "                                                             alpha = alpha).to(device))\n",
    "\n",
    "        elif model_name == \"ConvNet\":\n",
    "            model = nn.DataParallel(ConvNet(in_channels = input_length*2, \n",
    "                                            out_channels = 2,\n",
    "                                            hidden_dim = hidden_dim,\n",
    "                                            kernel_size = 3, \n",
    "                                            num_layers = num_layers).to(device))\n",
    "\n",
    "        elif model_name == \"RPP\":\n",
    "            model = nn.DataParallel(Rot_RPPNet(in_frames = input_length,\n",
    "                                               out_frames = 1,\n",
    "                                               hidden_dim = hidden_dim,\n",
    "                                               kernel_size = 3, \n",
    "                                               num_layers = num_layers,\n",
    "                                               N = 4).to(device))\n",
    "        elif model_name == \"Lift\":\n",
    "            model = nn.DataParallel(Lift_Rot_Expansion(in_frames = input_length,\n",
    "                                                       out_frames = 1, \n",
    "                                                       kernel_size = 3, \n",
    "                                                       encoder_hidden_dim = hidden_dim//2, \n",
    "                                                       backbone_hidden_dim = hidden_dim//2, \n",
    "                                                       N = 4).to(device))\n",
    "\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), learning_rate,betas=(0.9, 0.999), weight_decay=4e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma=decay_rate)\n",
    "        loss_fun = torch.nn.MSELoss()\n",
    "\n",
    "        ########################################################################################################################################################\n",
    "\n",
    "        train_rmse = []\n",
    "        valid_rmse = []\n",
    "        best_model = 0\n",
    "        checkpoint_path = 'figure_4_checkpoints'\n",
    "        for i in range(num_epoch):\n",
    "            start = time.time()\n",
    "\n",
    "            model.train()\n",
    "            train_rmse.append(train_epoch(train_loader, model, optimizer, loss_fun))\n",
    "\n",
    "            model.eval()\n",
    "            mse, preds, trues = eval_epoch(valid_loader, model, loss_fun)\n",
    "            valid_rmse.append(mse)\n",
    "\n",
    "            if valid_rmse[-1] < min_rmse:\n",
    "                min_rmse = valid_rmse[-1] \n",
    "                best_model = model\n",
    "\n",
    "            if i == num_epoch // 4 or i == num_epoch // 2 or i  == num_epoch // 4 * 3:\n",
    "                checkpoint = {\n",
    "                    'epoch' : i,\n",
    "                    'model_state_dict' : model.state_dict(),\n",
    "                    'optimizer_state_dict' : optimizer.state_dict(),\n",
    "                    'mse_validation_error' : mse\n",
    "                }\n",
    "                torch.save(checkpoint,checkpoint_path + '/' + model_name + '_checkpoint_' + str(int(i/num_epoch*100)))\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            # Early Stopping\n",
    "            if (len(train_rmse) > 50 and np.mean(valid_rmse[-5:]) >= np.mean(valid_rmse[-10:-5])):\n",
    "                    break       \n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch' : i,\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'mse_validation_error' : mse\n",
    "        }\n",
    "        torch.save(checkpoint,checkpoint_path + '/' + model_name + '_checkpoint_100')\n",
    "        equiv_errors = []\n",
    "        with torch.no_grad():\n",
    "            for xx, yy in test_loader:\n",
    "                xx = xx.to(device)\n",
    "                orig_pred = best_model(xx).reshape(-1, 2, xx.shape[-2], xx.shape[-1])\n",
    "\n",
    "                for angle in [np.pi/2, np.pi, np.pi/2*3]:\n",
    "                    rho_inp = rot_field(xx.reshape(-1, 2, xx.shape[-2], xx.shape[-1]), angle).to(device)\n",
    "                    rho_inp = rho_inp.reshape(1, -1, xx.shape[-2], xx.shape[-1])\n",
    "                    rho_inp_outs = best_model(rho_inp).reshape(-1, 2, xx.shape[-2], xx.shape[-1])\n",
    "                    equiv_errors.append(torch.mean(torch.abs(rho_inp_outs - rot_field(orig_pred, angle))).data.cpu())\n",
    "        equiv_error_lst.append(np.round(np.mean(equiv_errors),3))\n",
    "\n",
    "    print(\"{} Equiv Errors: | {}\".format(model_name, equiv_error_lst))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
